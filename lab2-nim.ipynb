{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright **`(c)`** 2022 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: ES\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The goal of the game is to **avoid** taking the last object.\n",
    "\n",
    "* Task2.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task2.2: An agent using evolved rules using ES\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab2` inside the course repo \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pprint import pprint, pformat\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from dataclasses import dataclass \n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample (and silly) startegies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    \"\"\"A completely random move\"\"\"\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabriele(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive(state: Nim) -> Nimply:\n",
    "    \"\"\"A strategy that can adapt its parameters\"\"\"\n",
    "    genome = {\"love_small\": 0.5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Returns the nim sum of the rows (binary rappresentation XOR)\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    tmp = np.array([tuple(int(x) for x in f\"{c:032b}\") for c in state.rows])\n",
    "    xor = tmp.sum(axis=0) % 2\n",
    "    return int(\"\".join(str(_) for _ in xor), base=2)\n",
    "\n",
    "# Create a dictionary that contains all possible (valid) moves\n",
    "def analize(raw: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = dict()\n",
    "    for ply in (Nimply(r, o) for r, c in enumerate(raw.rows) for o in range(1, c + 1)):\n",
    "        tmp = deepcopy(raw)\n",
    "        tmp.nimming(ply)\n",
    "        cooked[\"possible_moves\"][ply] = nim_sum(tmp)\n",
    "    return cooked\n",
    "\n",
    "# Returns the best moves that return a \n",
    "def optimal(state: Nim) -> Nimply:\n",
    "    analysis = analize(state)\n",
    "    logging.debug(f\"analysis:\\n{pformat(analysis)}\")\n",
    "    spicy_moves = [ply for ply, ns in analysis[\"possible_moves\"].items() if ns != 0]\n",
    "    if not spicy_moves:\n",
    "        spicy_moves = list(analysis[\"possible_moves\"].keys())\n",
    "    ply = random.choice(spicy_moves)\n",
    "    return ply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human -> allow a human to partecipate in the game \\\n",
    "Vinzgorithm -> an algorithm I tried to make knowing the rules of the game, performs quite well (modestly) \\\n",
    "Play -> replays the game a number of times specified in the function and returns the percentages of win of the 2 strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_rows(state:Nim):\n",
    "    n=np.count_nonzero(state.rows)\n",
    "    return n\n",
    "\n",
    "def human(state:Nim) ->Nimply:\n",
    "    while(True):\n",
    "        print(f\"{state} nim sum: {nim_sum(state)} and not zero columns: {get_number_of_rows(state)}\")\n",
    "        print(\"Write the row\")\n",
    "        r=int(input())\n",
    "        print(\"Write the number of elements to take\")\n",
    "        o=int(input())\n",
    "\n",
    "        if r<len(state.rows) and o<=state.rows[r]:\n",
    "            return Nimply(r,o)\n",
    "\n",
    "\n",
    "\n",
    "def vinzgorithm(state: Nim) -> Nimply:\n",
    "    ns=nim_sum(state)\n",
    "    \n",
    "    if(get_number_of_rows(state)>=2):\n",
    "        return Nimply(int(np.argmax(state.rows)), int(np.max(state.rows)))\n",
    "    else:\n",
    "        moves=[Nimply(r,c-1) for r,c in enumerate(state.rows) if(c>1)]\n",
    "    \n",
    "    logging.info(f\"len: {len(moves)} nin_sum: {ns}\")\n",
    "    if(len(moves)==0):\n",
    "        moves=[Nimply(n,o) for n,c in enumerate(state.rows) for o in range(1,c+1)]\n",
    "    ply=random.choice(moves)\n",
    "    return ply\n",
    "\n",
    "    \n",
    "def play(strategy1, strategy2, times, orig_nim=None):\n",
    "\n",
    "    if(orig_nim is None):\n",
    "        orig_nim=Nim(5)\n",
    "    assert callable(strategy1) , \"strategy1 is not a function\"\n",
    "    assert callable(strategy2) , \"strategy2 is not a function\"\n",
    "    strategy=(strategy1,strategy2)\n",
    "    win0=0\n",
    "    for _ in range(times):\n",
    "        nim=deepcopy(orig_nim)\n",
    "        logging.info(f\"init : {nim}\")\n",
    "        player = 0\n",
    "        while nim:\n",
    "            ply = strategy[player](nim)\n",
    "            logging.info(f\"ply: player {player} plays {ply}\")\n",
    "            nim.nimming(ply)\n",
    "            logging.info(f\"status: {nim}\")\n",
    "            player = 1 - player\n",
    "        logging.info(f\"status: Player {player} won!\")\n",
    "        if(player==0):\n",
    "            win0+=1\n",
    "    return win0/times, 1-win0/times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_state(rows:tuple):\n",
    "    possible_states=[rows]\n",
    "    temp=rows\n",
    "    for r in range(len(rows)):\n",
    "        temp_list=[]\n",
    "        for i in possible_states:\n",
    "            temp=list(i)\n",
    "            assert type(temp)!=int\n",
    "            \n",
    "            while (temp[r]>0):\n",
    "                temp[r]-=1\n",
    "                temp_list.append(tuple(temp))\n",
    "        possible_states.extend(temp_list)\n",
    "    \n",
    "    print(len(possible_states))\n",
    "    return possible_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Strategies\n",
    "Here the individual has a fitness value used to compare it with others which is the percentage of matches won against a chosen strategy\n",
    "The parameters are the probabilities of using different strategies, hopefully it converges towards the optimal or towards the vinzgorithm\n",
    "\n",
    "The softmax normalizes the outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    exp=np.exp(array)\n",
    "    return exp/np.sum(exp)\n",
    "\n",
    "\n",
    "class es_individual:\n",
    "    def __init__(self, *strategies, compare_strategy=optimal):\n",
    "        self.strategies=strategies\n",
    "        self.vec=np.array([random.random() for _ in range(len(strategies))])\n",
    "        self.vec=softmax(self.vec)\n",
    "        self.fitness_value=0\n",
    "        self.fitness(compare_strategy)\n",
    "\n",
    "    def strategy(self, state:Nim):\n",
    "        s=np.random.choice(self.strategies, p=self.vec, replace=False)\n",
    "        assert callable(s), f\"strategy not callable, type: {type(s)}\"\n",
    "        return s(state)\n",
    "    \n",
    "    def fitness(self, strategy=optimal):\n",
    "        _,win1=play(strategy, self.strategy, times=100)\n",
    "        if(win1>self.fitness_value):\n",
    "            self.fitness_value=win1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def tweak(self, strategy=optimal, sigma=1):\n",
    "        new_ind=deepcopy(self)\n",
    "        for i in range(len(self.vec)):\n",
    "            new_ind.vec[i]+=random.gauss(0,sigma)\n",
    "        new_ind.vec=softmax(new_ind.vec)\n",
    "        new_ind.fitness(strategy)\n",
    "        return new_ind\n",
    "\n",
    "def evolve_first_improv(epochs, nim_strategy=optimal) -> es_individual:\n",
    "    individual=es_individual(pure_random, vinzgorithm, optimal, gabriele)\n",
    "    sigma=1\n",
    "    for i in range(epochs):\n",
    "        sigma=(epochs-i)/epochs\n",
    "        new_individual=individual.tweak(optimal,sigma)\n",
    "        if new_individual.fitness_value>individual.fitness_value:\n",
    "            individual=new_individual\n",
    "    return individual\n",
    "\n",
    "def evolve_steepest(epochs,samples, nim_strategy=optimal) -> es_individual:\n",
    "    individual=es_individual(pure_random, vinzgorithm, optimal, gabriele)\n",
    "    for i in range(epochs):\n",
    "        sigma=(epochs-i)/epochs\n",
    "        new_individual=individual.tweak(nim_strategy, sigma)\n",
    "\n",
    "        for _ in range(samples-1):\n",
    "            temp_individual=new_individual.tweak(nim_strategy,sigma)\n",
    "            if temp_individual.fitness_value>new_individual.fitness_value:\n",
    "                new_individual=temp_individual\n",
    "\n",
    "        if new_individual.fitness_value>individual.fitness_value:\n",
    "            individual=new_individual\n",
    "\n",
    "    return individual\n",
    "\n",
    "def evolve_comma_lambda(epochs, samples, nim_strategy=optimal) -> es_individual:\n",
    "    individual=es_individual(pure_random, vinzgorithm, optimal, gabriele)\n",
    "    for i in range(epochs):\n",
    "        sigma=(epochs-i)/epochs\n",
    "        new_individual=individual.tweak(nim_strategy,sigma)\n",
    "\n",
    "        for _ in range(samples-1):\n",
    "            temp_individual=new_individual.tweak(nim_strategy, sigma)\n",
    "            if temp_individual.fitness_value>new_individual.fitness_value:\n",
    "                new_individual=temp_individual\n",
    "\n",
    "\n",
    "        individual=new_individual\n",
    "\n",
    "    return individual\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This was an attempt in making a very Brute Force approach at Genetic Algorithm\n",
    "(Before figuring out it was not the goal of the lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840\n"
     ]
    }
   ],
   "source": [
    "SIZE=5\n",
    "\n",
    "default_state=Nim(SIZE)\n",
    "\n",
    "possible_states=enum_state(default_state.rows)\n",
    "\n",
    "class Individual:\n",
    "    genome=dict()\n",
    "    phenotype=dict()\n",
    "    def __init__(self, strategy=optimal, calc_fitness=False):\n",
    "        global possible_states, default_state\n",
    "        \n",
    "        for t in possible_states:\n",
    "            moves=[Nimply(r,o) for r,c in enumerate(t) for o in range(1,c+1)]\n",
    "            self.phenotype[t]=0\n",
    "            if(len(moves)!=0):\n",
    "                self.genome[t]=random.choice(moves)\n",
    "            else:\n",
    "                self.genome[t]=Nimply(0,0)\n",
    "        \n",
    "        \n",
    "\n",
    "        if calc_fitness:\n",
    "            self.recalc_fitness(strategy)\n",
    "\n",
    "    def recalc_fitness(self, strategy=optimal):\n",
    "        global default_state\n",
    "        temp=deepcopy(self.phenotype)\n",
    "        self.fitness=play(strategy, self.strategy,100, default_state)[1]\n",
    "        if(self.fitness<0.5):\n",
    "            self.phenotype=temp\n",
    "        \n",
    "    def strategy(self, state:Nim):\n",
    "        self.phenotype[state.rows]+=1\n",
    "        return self.genome[state.rows]\n",
    "    \n",
    "    def reset_phenotype(self):\n",
    "        for i in self.phenotype.keys():\n",
    "            self.phenotype[i]=0\n",
    "        \n",
    "\n",
    "def crossover(individual1:Individual, individual2:Individual):\n",
    "    global possible_states\n",
    "    individual3=Individual()\n",
    "    individual4=Individual()\n",
    "    for t in possible_states:\n",
    "        if individual1.phenotype[t]==individual2.phenotype[t]:\n",
    "            if random.choice([True,False]):\n",
    "                individual3.genome[t]=individual1.genome[t]\n",
    "                individual4.genome[t]=individual2.genome[t]\n",
    "            else:\n",
    "                individual4.genome[t]=individual1.genome[t]\n",
    "                individual3.genome[t]=individual2.genome[t]\n",
    "        elif individual1.phenotype[t]>individual2.phenotype[t]:\n",
    "            individual3.genome[t]=individual1.genome[t]\n",
    "            individual4.genome[t]=individual1.genome[t]\n",
    "        else:\n",
    "            individual3.genome[t]=individual2.genome[t]\n",
    "            individual4.genome[t]=individual2.genome[t]\n",
    "\n",
    "    if(random.random()<0.05):\n",
    "        individual3=mutate(individual3)\n",
    "\n",
    "    individual3.recalc_fitness()\n",
    "    individual4.recalc_fitness()\n",
    "    \n",
    "    return individual3, individual4\n",
    "\n",
    "\n",
    "def mutate(individual : Individual):\n",
    "    global possible_states\n",
    "    \n",
    "    for t in possible_states:\n",
    "        moves=[Nimply(r,o) for r,c in enumerate(t) for o in range(1,c+1)]\n",
    "        if(len(moves)==0):\n",
    "            continue\n",
    "        if np.random.choice([True, False], p=[0.05,0.95]):\n",
    "            individual.genome[t]=random.choice(moves)\n",
    "    individual.recalc_fitness(optimal)\n",
    "    return individual\n",
    "\n",
    "\n",
    "def compare(individual1, individual2):\n",
    "    nim=deepcopy(default_state)\n",
    "\n",
    "    strategy=(individual1,individual2)\n",
    "    assert type(individual1)==Individual\n",
    "    assert type(individual2)==Individual\n",
    "    assert type(strategy[0])==Individual\n",
    "    player = 0\n",
    "    while nim:\n",
    "        ply = strategy[player].genome[nim.rows]\n",
    "        nim.nimming(ply)\n",
    "        player = 1 - player\n",
    "    loser=mutate(strategy[1-player])\n",
    "    loser.reset_phenotype()\n",
    "    return strategy[player], loser\n",
    "\n",
    "\n",
    "def eval(individual1, test):\n",
    "    nim=deepcopy(default_state)\n",
    "\n",
    "    strategy=(test, individual1.strategy)\n",
    "    player = 0\n",
    "    while nim:\n",
    "        ply = strategy[player](nim)\n",
    "        nim.nimming(ply)\n",
    "        player = 1 - player\n",
    "    return player==1\n",
    "\n",
    "def evolve(power_of_two:int):\n",
    "    n=2**power_of_two\n",
    "    individuals=[Individual() for _ in range(n)]\n",
    "    \n",
    "    while(True):\n",
    "        new_gen=[]\n",
    "        for i in range(0,n,2):\n",
    "            if i< n-1:\n",
    "                new_gen.extend(compare(individuals[i], individuals[i+1]))\n",
    "        \n",
    "        if(len(new_gen)==1):\n",
    "            return new_gen[0]\n",
    "        \n",
    "        individuals=[]\n",
    "        for j in range(0,len(new_gen),2):\n",
    "            individuals.extend(crossover(new_gen[j], new_gen[j+1]))\n",
    "            \n",
    "        n=len(individuals)\n",
    "\n",
    "def evolve_with_mutation(power_of_two:int):\n",
    "    n=2**power_of_two\n",
    "    individuals=[Individual(calc_fitness=True) for _ in range(n)]\n",
    "    \n",
    "    while(True):\n",
    "        individuals.sort(key=lambda ind: ind.fitness, reverse=True)\n",
    "        print([i.fitness for i in individuals])\n",
    "        winners=[]\n",
    "        losers=[]\n",
    "        for i in range(0,n,2):\n",
    "            if i<n-1:\n",
    "                win,lose=compare(individuals[i], individuals[i+1])\n",
    "                winners.append(win)\n",
    "                losers.append(lose)\n",
    "        \n",
    "\n",
    "        for i in range(10):    \n",
    "            if len(losers)>=1:\n",
    "                losers.remove(losers[random.randint(0,len(losers)-1)])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if(len(winners)==1):\n",
    "            return winners[0]\n",
    "        \n",
    "        individuals=[]\n",
    "        for j in range(0,len(winners),2):\n",
    "            if j<len(winners)-1:\n",
    "                individuals.extend(crossover(winners[j], winners[j+1]))\n",
    "            else:\n",
    "                individuals.append(winners[j])\n",
    "        individuals.extend(losers)\n",
    "        \n",
    "        n=len(individuals)\n",
    "\n",
    "def tournament(adv_strategy, n, max_epochs):\n",
    "    individuals=[Individual(calc_fitness=True) for _ in range(n)]\n",
    "    best=individuals[0]\n",
    "    for e in range(max_epochs):\n",
    "        individuals.sort(key=lambda ind: ind.fitness, reverse=True)\n",
    "        if(individuals[0].fitness>best.fitness):\n",
    "            best=deepcopy(individuals[0])\n",
    "            print(play(optimal, best.strategy, 100))\n",
    "        print(best.fitness,\" at the epoch: \",e )\n",
    "        #return individuals[0]\n",
    "        top=int(len(individuals)*0.2)\n",
    "        bot=len(individuals)-top\n",
    "\n",
    "        top_roulette=individuals[0:top]\n",
    "        bot_roulette=individuals[top:]\n",
    "\n",
    "        shuffle_top=random.sample(range(0,top), top)\n",
    "        shuffle_bot=random.sample(range(0,bot), bot)\n",
    "\n",
    "\n",
    "        new_gen=[]\n",
    "        for i in range(0,len(shuffle_top), 2):\n",
    "            if(i<len(shuffle_top)-1):\n",
    "                new_gen.extend(crossover(top_roulette[shuffle_top[i]], top_roulette[shuffle_top[i+1]]))\n",
    "            else:\n",
    "                new_gen.append(mutate(top_roulette[i]))\n",
    "        \n",
    "        for i in range(0,len(shuffle_bot), 2):\n",
    "            if(i<len(shuffle_bot)-1):\n",
    "                new_gen.extend(crossover(bot_roulette[shuffle_bot[i]], bot_roulette[shuffle_bot[i+1]]))\n",
    "            else:\n",
    "                new_gen.append(mutate(bot_roulette[i]))\n",
    "        \n",
    "        individuals=new_gen\n",
    "\n",
    "\n",
    "    individuals.sort(key=lambda ind: ind.fitness, reverse=True)\n",
    "    \n",
    "    if(individuals[0].fitness>best.fitness):\n",
    "        best=deepcopy(individuals[0])\n",
    "\n",
    "\n",
    "    worst=individuals[len(individuals)-1]\n",
    "    print(best.fitness, \" \", worst.fitness)\n",
    "    return best, worst\n",
    "\n",
    "        \n",
    "\n",
    "def evolve2(power_of_two:int):\n",
    "    n=2**power_of_two\n",
    "    individuals=[Individual() for _ in range(n)]\n",
    "    \n",
    "    while(True):\n",
    "        new_gen=[]\n",
    "        for i in range(0,n,2):\n",
    "            if(eval(individuals[i], optimal)):\n",
    "                new_gen.append(individuals[i])\n",
    "        \n",
    "        if(len(new_gen)==1):\n",
    "            return new_gen[0]\n",
    "        if(len(new_gen)==0):\n",
    "            return individuals[0]\n",
    "        \n",
    "        individuals=[]\n",
    "        for j in range(0,len(new_gen),2):\n",
    "            if j+1==len(new_gen):\n",
    "                individuals.append(new_gen[j])\n",
    "            else:\n",
    "                individuals.extend(crossover(new_gen[j], new_gen[j+1]))\n",
    "            \n",
    "        n=len(individuals)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Application\n",
    "Individuals are compared with the same strategy and with the same conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m nim \u001b[38;5;241m=\u001b[39m Nim(SIZE)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#i=Individual()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m last_man_standing, worst\u001b[38;5;241m=\u001b[39m\u001b[43mtournament\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m### GENETIC DOESN'T WORK WELL ###\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# individual=ESIndividual()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# individual.evolve(1000, nim, 1, optimal)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#individual1=evolve_first_improv(150)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#assert type(individual1) is es_individual\u001b[39;00m\n\u001b[0;32m     18\u001b[0m w0, w1\u001b[38;5;241m=\u001b[39mplay(optimal, last_man_standing\u001b[38;5;241m.\u001b[39mstrategy,\u001b[38;5;241m100\u001b[39m, nim)\n",
      "Cell \u001b[1;32mIn[11], line 166\u001b[0m, in \u001b[0;36mtournament\u001b[1;34m(adv_strategy, n, max_epochs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtournament\u001b[39m(adv_strategy, n, max_epochs):\n\u001b[1;32m--> 166\u001b[0m     individuals\u001b[38;5;241m=\u001b[39m[Individual(calc_fitness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m    167\u001b[0m     best\u001b[38;5;241m=\u001b[39mindividuals[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n",
      "Cell \u001b[1;32mIn[11], line 166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtournament\u001b[39m(adv_strategy, n, max_epochs):\n\u001b[1;32m--> 166\u001b[0m     individuals\u001b[38;5;241m=\u001b[39m[\u001b[43mIndividual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalc_fitness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[0;32m    167\u001b[0m     best\u001b[38;5;241m=\u001b[39mindividuals[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m, in \u001b[0;36mIndividual.__init__\u001b[1;34m(self, strategy, calc_fitness)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenome[t]\u001b[38;5;241m=\u001b[39mNimply(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calc_fitness:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecalc_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mIndividual.recalc_fitness\u001b[1;34m(self, strategy)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m default_state\n\u001b[0;32m     28\u001b[0m temp\u001b[38;5;241m=\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphenotype)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m=\u001b[39m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_state\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphenotype\u001b[38;5;241m=\u001b[39mtemp\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mplay\u001b[1;34m(strategy1, strategy2, times, orig_nim)\u001b[0m\n\u001b[0;32m     44\u001b[0m player \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m nim:\n\u001b[1;32m---> 46\u001b[0m     ply \u001b[38;5;241m=\u001b[39m \u001b[43mstrategy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mply: player \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m plays \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mply\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m     nim\u001b[38;5;241m.\u001b[39mnimming(ply)\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36moptimal\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimal\u001b[39m(state: Nim) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Nimply:\n\u001b[1;32m---> 21\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpformat(analysis)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m     spicy_moves \u001b[38;5;241m=\u001b[39m [ply \u001b[38;5;28;01mfor\u001b[39;00m ply, ns \u001b[38;5;129;01min\u001b[39;00m analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpossible_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36manalize\u001b[1;34m(raw)\u001b[0m\n\u001b[0;32m     11\u001b[0m cooked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     12\u001b[0m cooked[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpossible_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ply \u001b[38;5;129;01min\u001b[39;00m (Nimply(r, o) \u001b[38;5;28;01mfor\u001b[39;00m r, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(raw\u001b[38;5;241m.\u001b[39mrows) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, c \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     14\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m deepcopy(raw)\n\u001b[0;32m     15\u001b[0m     tmp\u001b[38;5;241m.\u001b[39mnimming(ply)\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m cooked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     12\u001b[0m cooked[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpossible_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ply \u001b[38;5;129;01min\u001b[39;00m (Nimply(r, o) \u001b[38;5;28;01mfor\u001b[39;00m r, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(raw\u001b[38;5;241m.\u001b[39mrows) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, c \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     14\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m deepcopy(raw)\n\u001b[0;32m     15\u001b[0m     tmp\u001b[38;5;241m.\u001b[39mnimming(ply)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.disable(logging.INFO)\n",
    "strategy = (vinzgorithm, optimal)\n",
    "\n",
    "nim = Nim(SIZE)\n",
    "\n",
    "#i=Individual()\n",
    "last_man_standing, worst=tournament(optimal, 100,50)\n",
    "\n",
    "\n",
    "### GENETIC DOESN'T WORK WELL ###\n",
    "# individual=ESIndividual()\n",
    "# individual.evolve(1000, nim, 1, optimal)\n",
    "\n",
    "### ES HOPE IT WORKS ###\n",
    "#individual1=evolve_first_improv(150)\n",
    "#assert type(individual1) is es_individual\n",
    "w0, w1=play(optimal, last_man_standing.strategy,100, nim)\n",
    "print(f\"Player 0 won: {w0*100}% of times Individual1 won: {w1*100}% of times\")\n",
    "\n",
    "w0, w1=play(optimal, worst.strategy,100, nim)\n",
    "print(f\"Player 0 won: {w0*100}% of times Individual1 won: {w1*100}% of times\")\n",
    "\n",
    "\n",
    "individual2=evolve_steepest(150,6)\n",
    "w0, w1=play(optimal, individual2.strategy, 100, nim)\n",
    "print(f\"Player 0 won: {w0*100}% of times Individual2 won: {w1*100}% of times\")\n",
    "\n",
    "individual3=evolve_comma_lambda(150,6)\n",
    "w0, w1=play(optimal, individual3.strategy, 100, nim)\n",
    "print(f\"Player 0 won: {w0*100}% of times Individual3 won: {w1*100}% of times\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('ci22-dPIXJ0_o-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "10197e8e2f2aa67e2c349105091c77f4cd384fce4877865f002d9ec653f96bc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
